{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PDE Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Burger's equation: u_t + u*u_x = 0\n",
    "def pde_res(model, x, t):\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        tape.watch([x, t])\n",
    "        u = model(tf.concat([x, t], axis=1))\n",
    "        u_x = tape.gradient(u, x)\n",
    "        u_t = tape.gradient(u, t)\n",
    "    del tape\n",
    "    pde_residual = u_t + u*u_x\n",
    "    \n",
    "    return tf.reduce_mean(tf.square(pde_residual))\n",
    "\n",
    "def u_0(x):\n",
    "    return np.sin(x)  # u_0 = sin(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PINN(tf.keras.Model):\n",
    "    def __init__(self, layers):\n",
    "        super(PINN, self).__init__()\n",
    "        self.hidden_layers = []\n",
    "        for units in layers:\n",
    "            self.hidden_layers.append(tf.keras.layers.Dense(units, activation='tanh'))\n",
    "        self.output_layer = tf.keras.layers.Dense(1)\n",
    "\n",
    "    def call(self, x):\n",
    "        z = x\n",
    "        for layer in self.hidden_layers:\n",
    "            z = layer(z)\n",
    "        output = self.output_layer(z)\n",
    "        return output\n",
    "    \n",
    "def to_tf_tensor(array):\n",
    "    return tf.convert_to_tensor(array, dtype=tf.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Physics Inspired Loss and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(model, x, t, u_true, pde_res):\n",
    "    u_H = model(tf.concat([x, t], axis=1))\n",
    "    data_loss = tf.reduce_mean(tf.square(u_H - u_true))\n",
    "    \n",
    "    pde_loss = pde_res(model, x, t)\n",
    "    \n",
    "    return data_loss + pde_loss\n",
    "\n",
    "\n",
    "def train(model, x, t, u_true, pde_res, epochs, learning_rate):\n",
    "    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    for epoch in range(epochs):\n",
    "        with tf.GradientTape() as tape:\n",
    "            losss = loss(model, x, t, u_true, pde_res)\n",
    "        gradients = tape.gradient(losss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "        \n",
    "        print(f\"Epoch {epoch}, Loss: {losss.numpy()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(num_x, num_t, xmin, xmax, tmin, tmax):\n",
    "    x = np.linspace(xmin, xmax, num_x)\n",
    "    t = np.linspace(tmin, tmax, num_t)\n",
    "   \n",
    "    X, T = np.meshgrid(x, t)\n",
    "    X = X.flatten()[:, None]\n",
    "    T = T.flatten()[:, None]\n",
    "    \n",
    "    U = u_0(X)  \n",
    "\n",
    "    return to_tf_tensor(X), to_tf_tensor(T), to_tf_tensor(U)\n",
    "\n",
    "\n",
    "# Parameters\n",
    "\n",
    "layers = [128, 128, 128, 128]\n",
    "learning_rate = 0.001\n",
    "epochs = 512\n",
    "\n",
    "x_points = 512\n",
    "t_points = 200\n",
    "xmin, xmax = -1, 1\n",
    "tmin, tmax = 0, 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 0.3337097465991974\n",
      "Epoch 1, Loss: 0.14662709832191467\n",
      "Epoch 2, Loss: 0.06681917607784271\n",
      "Epoch 3, Loss: 0.16252253949642181\n",
      "Epoch 4, Loss: 0.12553945183753967\n",
      "Epoch 5, Loss: 0.08354584127664566\n",
      "Epoch 6, Loss: 0.06569784879684448\n",
      "Epoch 7, Loss: 0.07669568806886673\n",
      "Epoch 8, Loss: 0.09336361289024353\n",
      "Epoch 9, Loss: 0.09864979237318039\n",
      "Epoch 10, Loss: 0.09599123150110245\n",
      "Epoch 11, Loss: 0.0903782993555069\n",
      "Epoch 12, Loss: 0.0810958668589592\n",
      "Epoch 13, Loss: 0.06980000436306\n",
      "Epoch 14, Loss: 0.0633203461766243\n",
      "Epoch 15, Loss: 0.0644429475069046\n",
      "Epoch 16, Loss: 0.06843984872102737\n",
      "Epoch 17, Loss: 0.07229151576757431\n",
      "Epoch 18, Loss: 0.07426580786705017\n",
      "Epoch 19, Loss: 0.0706639289855957\n",
      "Epoch 20, Loss: 0.06455948203802109\n",
      "Epoch 21, Loss: 0.06138940155506134\n",
      "Epoch 22, Loss: 0.060645196586847305\n",
      "Epoch 23, Loss: 0.06088530644774437\n",
      "Epoch 24, Loss: 0.062475256621837616\n",
      "Epoch 25, Loss: 0.063908651471138\n",
      "Epoch 26, Loss: 0.06339363753795624\n",
      "Epoch 27, Loss: 0.06187715381383896\n",
      "Epoch 28, Loss: 0.06009310483932495\n",
      "Epoch 29, Loss: 0.05752933770418167\n",
      "Epoch 30, Loss: 0.05546383559703827\n",
      "Epoch 31, Loss: 0.05485713481903076\n",
      "Epoch 32, Loss: 0.054487958550453186\n",
      "Epoch 33, Loss: 0.05449765548110008\n",
      "Epoch 34, Loss: 0.05421727895736694\n",
      "Epoch 35, Loss: 0.05253065750002861\n",
      "Epoch 36, Loss: 0.05078551918268204\n",
      "Epoch 37, Loss: 0.048840995877981186\n",
      "Epoch 38, Loss: 0.04746844992041588\n",
      "Epoch 39, Loss: 0.04684421047568321\n",
      "Epoch 40, Loss: 0.04612327739596367\n",
      "Epoch 41, Loss: 0.04558540880680084\n",
      "Epoch 42, Loss: 0.044265180826187134\n",
      "Epoch 43, Loss: 0.042874619364738464\n",
      "Epoch 44, Loss: 0.04112376272678375\n",
      "Epoch 45, Loss: 0.039878178387880325\n",
      "Epoch 46, Loss: 0.03885336592793465\n",
      "Epoch 47, Loss: 0.03833439201116562\n",
      "Epoch 48, Loss: 0.03747232258319855\n",
      "Epoch 49, Loss: 0.03657640144228935\n",
      "Epoch 50, Loss: 0.035386569797992706\n",
      "Epoch 51, Loss: 0.034538887441158295\n",
      "Epoch 52, Loss: 0.034067749977111816\n",
      "Epoch 53, Loss: 0.03356432914733887\n",
      "Epoch 54, Loss: 0.033033337444067\n",
      "Epoch 55, Loss: 0.032198913395404816\n",
      "Epoch 56, Loss: 0.03124929964542389\n",
      "Epoch 57, Loss: 0.030591189861297607\n",
      "Epoch 58, Loss: 0.030092870816588402\n",
      "Epoch 59, Loss: 0.02941138669848442\n",
      "Epoch 60, Loss: 0.028498178347945213\n",
      "Epoch 61, Loss: 0.027589358389377594\n",
      "Epoch 62, Loss: 0.02686578407883644\n",
      "Epoch 63, Loss: 0.026263274252414703\n",
      "Epoch 64, Loss: 0.025623515248298645\n",
      "Epoch 65, Loss: 0.024936003610491753\n",
      "Epoch 66, Loss: 0.024409979581832886\n",
      "Epoch 67, Loss: 0.024353373795747757\n",
      "Epoch 68, Loss: 0.025273166596889496\n",
      "Epoch 69, Loss: 0.027701687067747116\n",
      "Epoch 70, Loss: 0.029730498790740967\n",
      "Epoch 71, Loss: 0.025985434651374817\n",
      "Epoch 72, Loss: 0.02100609801709652\n",
      "Epoch 73, Loss: 0.023066096007823944\n",
      "Epoch 74, Loss: 0.02536153793334961\n",
      "Epoch 75, Loss: 0.02151154726743698\n",
      "Epoch 76, Loss: 0.01956033706665039\n",
      "Epoch 77, Loss: 0.022338315844535828\n",
      "Epoch 78, Loss: 0.021866969764232635\n",
      "Epoch 79, Loss: 0.018507786095142365\n",
      "Epoch 80, Loss: 0.018974274396896362\n",
      "Epoch 81, Loss: 0.020528893917798996\n",
      "Epoch 82, Loss: 0.01856149733066559\n",
      "Epoch 83, Loss: 0.01694413647055626\n",
      "Epoch 84, Loss: 0.01818118244409561\n",
      "Epoch 85, Loss: 0.018381267786026\n",
      "Epoch 86, Loss: 0.01652195304632187\n",
      "Epoch 87, Loss: 0.015875808894634247\n",
      "Epoch 88, Loss: 0.01686086691915989\n",
      "Epoch 89, Loss: 0.016918275505304337\n",
      "Epoch 90, Loss: 0.015589270740747452\n",
      "Epoch 91, Loss: 0.014890528284013271\n",
      "Epoch 92, Loss: 0.015449506230652332\n",
      "Epoch 93, Loss: 0.015868129208683968\n",
      "Epoch 94, Loss: 0.015291455201804638\n",
      "Epoch 95, Loss: 0.014420135878026485\n",
      "Epoch 96, Loss: 0.014202929101884365\n",
      "Epoch 97, Loss: 0.014595823362469673\n",
      "Epoch 98, Loss: 0.014948774129152298\n",
      "Epoch 99, Loss: 0.014826903119683266\n",
      "Epoch 100, Loss: 0.01430394034832716\n",
      "Epoch 101, Loss: 0.013789900578558445\n",
      "Epoch 102, Loss: 0.013591533526778221\n",
      "Epoch 103, Loss: 0.0137212248519063\n",
      "Epoch 104, Loss: 0.014014957472682\n",
      "Epoch 105, Loss: 0.014307182282209396\n",
      "Epoch 106, Loss: 0.014505233615636826\n",
      "Epoch 107, Loss: 0.014552155509591103\n",
      "Epoch 108, Loss: 0.014430670067667961\n",
      "Epoch 109, Loss: 0.014152782037854195\n",
      "Epoch 110, Loss: 0.013804749585688114\n",
      "Epoch 111, Loss: 0.01348390243947506\n",
      "Epoch 112, Loss: 0.013253972865641117\n",
      "Epoch 113, Loss: 0.013122125528752804\n",
      "Epoch 114, Loss: 0.013072030618786812\n",
      "Epoch 115, Loss: 0.0130874989554286\n",
      "Epoch 116, Loss: 0.013157658278942108\n",
      "Epoch 117, Loss: 0.013284409418702126\n",
      "Epoch 118, Loss: 0.013495693914592266\n",
      "Epoch 119, Loss: 0.013853976503014565\n",
      "Epoch 120, Loss: 0.014445580542087555\n",
      "Epoch 121, Loss: 0.015354677103459835\n",
      "Epoch 122, Loss: 0.01649590954184532\n",
      "Epoch 123, Loss: 0.01737889274954796\n",
      "Epoch 124, Loss: 0.01705339550971985\n",
      "Epoch 125, Loss: 0.015227816998958588\n",
      "Epoch 126, Loss: 0.013227620162069798\n",
      "Epoch 127, Loss: 0.012779362499713898\n",
      "Epoch 128, Loss: 0.013859267346560955\n",
      "Epoch 129, Loss: 0.014904260635375977\n",
      "Epoch 130, Loss: 0.014600599184632301\n",
      "Epoch 131, Loss: 0.013305997475981712\n",
      "Epoch 132, Loss: 0.012594535015523434\n",
      "Epoch 133, Loss: 0.013099384494125843\n",
      "Epoch 134, Loss: 0.013816367834806442\n",
      "Epoch 135, Loss: 0.013661476783454418\n",
      "Epoch 136, Loss: 0.01286862138658762\n",
      "Epoch 137, Loss: 0.012488126754760742\n",
      "Epoch 138, Loss: 0.012847481295466423\n",
      "Epoch 139, Loss: 0.013242186047136784\n",
      "Epoch 140, Loss: 0.013054721057415009\n",
      "Epoch 141, Loss: 0.012563148513436317\n",
      "Epoch 142, Loss: 0.012410764582455158\n",
      "Epoch 143, Loss: 0.012672411277890205\n",
      "Epoch 144, Loss: 0.01287300605326891\n",
      "Epoch 145, Loss: 0.012712056748569012\n",
      "Epoch 146, Loss: 0.0124099375680089\n",
      "Epoch 147, Loss: 0.012325595133006573\n",
      "Epoch 148, Loss: 0.012478305026888847\n",
      "Epoch 149, Loss: 0.0125972805544734\n",
      "Epoch 150, Loss: 0.012510394677519798\n",
      "Epoch 151, Loss: 0.012323766946792603\n",
      "Epoch 152, Loss: 0.012237895280122757\n",
      "Epoch 153, Loss: 0.012300697155296803\n",
      "Epoch 154, Loss: 0.012388769537210464\n",
      "Epoch 155, Loss: 0.012378232553601265\n",
      "Epoch 156, Loss: 0.01227565761655569\n",
      "Epoch 157, Loss: 0.012181208468973637\n",
      "Epoch 158, Loss: 0.012166142463684082\n",
      "Epoch 159, Loss: 0.012211059220135212\n",
      "Epoch 160, Loss: 0.01224722433835268\n",
      "Epoch 161, Loss: 0.012229862622916698\n",
      "Epoch 162, Loss: 0.012169494293630123\n",
      "Epoch 163, Loss: 0.012110238894820213\n",
      "Epoch 164, Loss: 0.012086117640137672\n",
      "Epoch 165, Loss: 0.012097206898033619\n",
      "Epoch 166, Loss: 0.012118467129766941\n",
      "Epoch 167, Loss: 0.012124456465244293\n",
      "Epoch 168, Loss: 0.012106295675039291\n",
      "Epoch 169, Loss: 0.012071806937456131\n",
      "Epoch 170, Loss: 0.012036518193781376\n",
      "Epoch 171, Loss: 0.012012904509902\n",
      "Epoch 172, Loss: 0.01200447604060173\n",
      "Epoch 173, Loss: 0.012006494216620922\n",
      "Epoch 174, Loss: 0.012011215090751648\n",
      "Epoch 175, Loss: 0.012012391351163387\n",
      "Epoch 176, Loss: 0.012006785720586777\n",
      "Epoch 177, Loss: 0.011994407512247562\n",
      "Epoch 178, Loss: 0.01197756826877594\n",
      "Epoch 179, Loss: 0.011959205381572247\n",
      "Epoch 180, Loss: 0.0119414571672678\n",
      "Epoch 181, Loss: 0.011925600469112396\n",
      "Epoch 182, Loss: 0.0119121503084898\n",
      "Epoch 183, Loss: 0.011900927871465683\n",
      "Epoch 184, Loss: 0.011891442351043224\n",
      "Epoch 185, Loss: 0.011883309110999107\n",
      "Epoch 186, Loss: 0.011876247823238373\n",
      "Epoch 187, Loss: 0.011870088055729866\n",
      "Epoch 188, Loss: 0.011864975094795227\n",
      "Epoch 189, Loss: 0.011861424893140793\n",
      "Epoch 190, Loss: 0.01186034083366394\n",
      "Epoch 191, Loss: 0.011863422580063343\n",
      "Epoch 192, Loss: 0.011873876675963402\n",
      "Epoch 193, Loss: 0.01189751923084259\n",
      "Epoch 194, Loss: 0.011945253238081932\n",
      "Epoch 195, Loss: 0.012037372216582298\n",
      "Epoch 196, Loss: 0.012212558649480343\n",
      "Epoch 197, Loss: 0.012540626339614391\n",
      "Epoch 198, Loss: 0.013147053308784962\n",
      "Epoch 199, Loss: 0.014212892390787601\n",
      "Epoch 200, Loss: 0.015927892178297043\n",
      "Epoch 201, Loss: 0.0181039460003376\n",
      "Epoch 202, Loss: 0.01968429423868656\n",
      "Epoch 203, Loss: 0.018644895404577255\n",
      "Epoch 204, Loss: 0.014926193282008171\n",
      "Epoch 205, Loss: 0.011919951997697353\n",
      "Epoch 206, Loss: 0.012546276673674583\n",
      "Epoch 207, Loss: 0.014968356117606163\n",
      "Epoch 208, Loss: 0.015283191576600075\n",
      "Epoch 209, Loss: 0.013023225590586662\n",
      "Epoch 210, Loss: 0.011698927730321884\n",
      "Epoch 211, Loss: 0.012929035350680351\n",
      "Epoch 212, Loss: 0.013933798298239708\n",
      "Epoch 213, Loss: 0.012799403630197048\n",
      "Epoch 214, Loss: 0.011658486910164356\n",
      "Epoch 215, Loss: 0.012310474179685116\n",
      "Epoch 216, Loss: 0.013045838102698326\n",
      "Epoch 217, Loss: 0.012328187003731728\n",
      "Epoch 218, Loss: 0.011613242328166962\n",
      "Epoch 219, Loss: 0.01211552508175373\n",
      "Epoch 220, Loss: 0.012531567364931107\n",
      "Epoch 221, Loss: 0.011968179605901241\n",
      "Epoch 222, Loss: 0.011580665595829487\n",
      "Epoch 223, Loss: 0.011979302391409874\n",
      "Epoch 224, Loss: 0.012150589376688004\n",
      "Epoch 225, Loss: 0.011723596602678299\n",
      "Epoch 226, Loss: 0.011564262211322784\n",
      "Epoch 227, Loss: 0.011862059123814106\n",
      "Epoch 228, Loss: 0.011893102899193764\n",
      "Epoch 229, Loss: 0.011587908491492271\n",
      "Epoch 230, Loss: 0.011542200110852718\n",
      "Epoch 231, Loss: 0.011742531321942806\n",
      "Epoch 232, Loss: 0.011710253544151783\n",
      "Epoch 233, Loss: 0.011505589820444584\n",
      "Epoch 234, Loss: 0.011505899019539356\n",
      "Epoch 235, Loss: 0.011632553301751614\n",
      "Epoch 236, Loss: 0.011586860753595829\n",
      "Epoch 237, Loss: 0.011450322344899178\n",
      "Epoch 238, Loss: 0.011460304260253906\n",
      "Epoch 239, Loss: 0.011539466679096222\n",
      "Epoch 240, Loss: 0.011499371379613876\n",
      "Epoch 241, Loss: 0.011405741795897484\n",
      "Epoch 242, Loss: 0.011408951133489609\n",
      "Epoch 243, Loss: 0.01145786140114069\n",
      "Epoch 244, Loss: 0.011431263759732246\n",
      "Epoch 245, Loss: 0.011365089565515518\n",
      "Epoch 246, Loss: 0.011357497423887253\n",
      "Epoch 247, Loss: 0.011387074366211891\n",
      "Epoch 248, Loss: 0.011373926885426044\n",
      "Epoch 249, Loss: 0.011326086707413197\n",
      "Epoch 250, Loss: 0.011308211833238602\n",
      "Epoch 251, Loss: 0.011323327198624611\n",
      "Epoch 252, Loss: 0.011320064775645733\n",
      "Epoch 253, Loss: 0.01128690131008625\n",
      "Epoch 254, Loss: 0.011262762360274792\n",
      "Epoch 255, Loss: 0.011264699511229992\n",
      "Epoch 256, Loss: 0.011265642940998077\n",
      "Epoch 257, Loss: 0.01124568097293377\n",
      "Epoch 258, Loss: 0.011220790445804596\n",
      "Epoch 259, Loss: 0.011211172677576542\n",
      "Epoch 260, Loss: 0.011210400611162186\n",
      "Epoch 261, Loss: 0.01120027992874384\n",
      "Epoch 262, Loss: 0.011179743334650993\n",
      "Epoch 263, Loss: 0.011162951588630676\n",
      "Epoch 264, Loss: 0.01115590799599886\n",
      "Epoch 265, Loss: 0.011149651370942593\n",
      "Epoch 266, Loss: 0.011135921813547611\n",
      "Epoch 267, Loss: 0.011118222028017044\n",
      "Epoch 268, Loss: 0.011104545556008816\n",
      "Epoch 269, Loss: 0.011095922440290451\n",
      "Epoch 270, Loss: 0.011086472310125828\n",
      "Epoch 271, Loss: 0.011072401888668537\n",
      "Epoch 272, Loss: 0.011056506074965\n",
      "Epoch 273, Loss: 0.011043168604373932\n",
      "Epoch 274, Loss: 0.01103258691728115\n",
      "Epoch 275, Loss: 0.011021475307643414\n",
      "Epoch 276, Loss: 0.011007705703377724\n",
      "Epoch 277, Loss: 0.010992576368153095\n",
      "Epoch 278, Loss: 0.010978596284985542\n",
      "Epoch 279, Loss: 0.010966300964355469\n",
      "Epoch 280, Loss: 0.010954056866466999\n",
      "Epoch 281, Loss: 0.01094038411974907\n",
      "Epoch 282, Loss: 0.010925518348813057\n",
      "Epoch 283, Loss: 0.01091077458113432\n",
      "Epoch 284, Loss: 0.01089694444090128\n",
      "Epoch 285, Loss: 0.01088353618979454\n",
      "Epoch 286, Loss: 0.010869570076465607\n",
      "Epoch 287, Loss: 0.010854700580239296\n",
      "Epoch 288, Loss: 0.010839378461241722\n",
      "Epoch 289, Loss: 0.010824263095855713\n",
      "Epoch 290, Loss: 0.01080953236669302\n",
      "Epoch 291, Loss: 0.01079479232430458\n",
      "Epoch 292, Loss: 0.010779602453112602\n",
      "Epoch 293, Loss: 0.0107638631016016\n",
      "Epoch 294, Loss: 0.01074784155935049\n",
      "Epoch 295, Loss: 0.010731849819421768\n",
      "Epoch 296, Loss: 0.010715956799685955\n",
      "Epoch 297, Loss: 0.010699987411499023\n",
      "Epoch 298, Loss: 0.010683716274797916\n",
      "Epoch 299, Loss: 0.010667070746421814\n",
      "Epoch 300, Loss: 0.010650149546563625\n",
      "Epoch 301, Loss: 0.010633116587996483\n",
      "Epoch 302, Loss: 0.010616039857268333\n",
      "Epoch 303, Loss: 0.01059887744486332\n",
      "Epoch 304, Loss: 0.010581526905298233\n",
      "Epoch 305, Loss: 0.010563906282186508\n",
      "Epoch 306, Loss: 0.010546035133302212\n",
      "Epoch 307, Loss: 0.010527963750064373\n",
      "Epoch 308, Loss: 0.010509763844311237\n",
      "Epoch 309, Loss: 0.010491454042494297\n",
      "Epoch 310, Loss: 0.010473014786839485\n",
      "Epoch 311, Loss: 0.01045440137386322\n",
      "Epoch 312, Loss: 0.010435588657855988\n",
      "Epoch 313, Loss: 0.010416578501462936\n",
      "Epoch 314, Loss: 0.010397395119071007\n",
      "Epoch 315, Loss: 0.010378074832260609\n",
      "Epoch 316, Loss: 0.01035863347351551\n",
      "Epoch 317, Loss: 0.010339071974158287\n",
      "Epoch 318, Loss: 0.010319380089640617\n",
      "Epoch 319, Loss: 0.01029954757541418\n",
      "Epoch 320, Loss: 0.010279571637511253\n",
      "Epoch 321, Loss: 0.01025946345180273\n",
      "Epoch 322, Loss: 0.010239234194159508\n",
      "Epoch 323, Loss: 0.010218900628387928\n",
      "Epoch 324, Loss: 0.010198479518294334\n",
      "Epoch 325, Loss: 0.010177967138588428\n",
      "Epoch 326, Loss: 0.010157374665141106\n",
      "Epoch 327, Loss: 0.010136697441339493\n",
      "Epoch 328, Loss: 0.010115939192473888\n",
      "Epoch 329, Loss: 0.01009510550647974\n",
      "Epoch 330, Loss: 0.010074201971292496\n",
      "Epoch 331, Loss: 0.0100532416254282\n",
      "Epoch 332, Loss: 0.010032227262854576\n",
      "Epoch 333, Loss: 0.01001117005944252\n",
      "Epoch 334, Loss: 0.009990074671804905\n",
      "Epoch 335, Loss: 0.00996894296258688\n",
      "Epoch 336, Loss: 0.009947781451046467\n",
      "Epoch 337, Loss: 0.009926590137183666\n",
      "Epoch 338, Loss: 0.009905371814966202\n",
      "Epoch 339, Loss: 0.009884130209684372\n",
      "Epoch 340, Loss: 0.00986286997795105\n",
      "Epoch 341, Loss: 0.00984159018844366\n",
      "Epoch 342, Loss: 0.009820295497775078\n",
      "Epoch 343, Loss: 0.009798990562558174\n",
      "Epoch 344, Loss: 0.0097776735201478\n",
      "Epoch 345, Loss: 0.009756349958479404\n",
      "Epoch 346, Loss: 0.009735018946230412\n",
      "Epoch 347, Loss: 0.009713681414723396\n",
      "Epoch 348, Loss: 0.009692339226603508\n",
      "Epoch 349, Loss: 0.009670993313193321\n",
      "Epoch 350, Loss: 0.009649643674492836\n",
      "Epoch 351, Loss: 0.009628294967114925\n",
      "Epoch 352, Loss: 0.00960694532841444\n",
      "Epoch 353, Loss: 0.009585598483681679\n",
      "Epoch 354, Loss: 0.009564257226884365\n",
      "Epoch 355, Loss: 0.0095429178327322\n",
      "Epoch 356, Loss: 0.009521590545773506\n",
      "Epoch 357, Loss: 0.009500276297330856\n",
      "Epoch 358, Loss: 0.0094789769500494\n",
      "Epoch 359, Loss: 0.009457697160542011\n",
      "Epoch 360, Loss: 0.009436442516744137\n",
      "Epoch 361, Loss: 0.009415216743946075\n",
      "Epoch 362, Loss: 0.009394025430083275\n",
      "Epoch 363, Loss: 0.009372873231768608\n",
      "Epoch 364, Loss: 0.009351768530905247\n",
      "Epoch 365, Loss: 0.009330717846751213\n",
      "Epoch 366, Loss: 0.009309729561209679\n",
      "Epoch 367, Loss: 0.009288815781474113\n",
      "Epoch 368, Loss: 0.009267984889447689\n",
      "Epoch 369, Loss: 0.009247253648936749\n",
      "Epoch 370, Loss: 0.009226644411683083\n",
      "Epoch 371, Loss: 0.009206187911331654\n",
      "Epoch 372, Loss: 0.00918593630194664\n",
      "Epoch 373, Loss: 0.009165968745946884\n",
      "Epoch 374, Loss: 0.009146425873041153\n",
      "Epoch 375, Loss: 0.009127562865614891\n",
      "Epoch 376, Loss: 0.00910983793437481\n",
      "Epoch 377, Loss: 0.009094111621379852\n",
      "Epoch 378, Loss: 0.009082025848329067\n",
      "Epoch 379, Loss: 0.009076742455363274\n",
      "Epoch 380, Loss: 0.00908446591347456\n",
      "Epoch 381, Loss: 0.009117493405938148\n",
      "Epoch 382, Loss: 0.009200239554047585\n",
      "Epoch 383, Loss: 0.009381613694131374\n",
      "Epoch 384, Loss: 0.009756459854543209\n",
      "Epoch 385, Loss: 0.010504644364118576\n",
      "Epoch 386, Loss: 0.011916222982108593\n",
      "Epoch 387, Loss: 0.014347288757562637\n",
      "Epoch 388, Loss: 0.017683198675513268\n",
      "Epoch 389, Loss: 0.020398281514644623\n",
      "Epoch 390, Loss: 0.019055718556046486\n",
      "Epoch 391, Loss: 0.013315455988049507\n",
      "Epoch 392, Loss: 0.008980557322502136\n",
      "Epoch 393, Loss: 0.010653236880898476\n",
      "Epoch 394, Loss: 0.01404604222625494\n",
      "Epoch 395, Loss: 0.01281124260276556\n",
      "Epoch 396, Loss: 0.009200853295624256\n",
      "Epoch 397, Loss: 0.009701947681605816\n",
      "Epoch 398, Loss: 0.012052319943904877\n",
      "Epoch 399, Loss: 0.010951770469546318\n",
      "Epoch 400, Loss: 0.008794366382062435\n",
      "Epoch 401, Loss: 0.009954815730452538\n",
      "Epoch 402, Loss: 0.011003602296113968\n",
      "Epoch 403, Loss: 0.00941937044262886\n",
      "Epoch 404, Loss: 0.008876108564436436\n",
      "Epoch 405, Loss: 0.01013156771659851\n",
      "Epoch 406, Loss: 0.009748523123562336\n",
      "Epoch 407, Loss: 0.008697076700627804\n",
      "Epoch 408, Loss: 0.009315161034464836\n",
      "Epoch 409, Loss: 0.009632204659283161\n",
      "Epoch 410, Loss: 0.008796490728855133\n",
      "Epoch 411, Loss: 0.008846277371048927\n",
      "Epoch 412, Loss: 0.009351763874292374\n",
      "Epoch 413, Loss: 0.008879683911800385\n",
      "Epoch 414, Loss: 0.008647737093269825\n",
      "Epoch 415, Loss: 0.0090568158775568\n",
      "Epoch 416, Loss: 0.0089203380048275\n",
      "Epoch 417, Loss: 0.008574592880904675\n",
      "Epoch 418, Loss: 0.008825594559311867\n",
      "Epoch 419, Loss: 0.008884234353899956\n",
      "Epoch 420, Loss: 0.008591271936893463\n",
      "Epoch 421, Loss: 0.008642823435366154\n",
      "Epoch 422, Loss: 0.00880730152130127\n",
      "Epoch 423, Loss: 0.008615924045443535\n",
      "Epoch 424, Loss: 0.008546102792024612\n",
      "Epoch 425, Loss: 0.008688329719007015\n",
      "Epoch 426, Loss: 0.008634042926132679\n",
      "Epoch 427, Loss: 0.008503995835781097\n",
      "Epoch 428, Loss: 0.008584476076066494\n",
      "Epoch 429, Loss: 0.008612146601080894\n",
      "Epoch 430, Loss: 0.008506585843861103\n",
      "Epoch 431, Loss: 0.00850449874997139\n",
      "Epoch 432, Loss: 0.008567809127271175\n",
      "Epoch 433, Loss: 0.00851251371204853\n",
      "Epoch 434, Loss: 0.00846578273922205\n",
      "Epoch 435, Loss: 0.008508178405463696\n",
      "Epoch 436, Loss: 0.008509754203259945\n",
      "Epoch 437, Loss: 0.008454110473394394\n",
      "Epoch 438, Loss: 0.008459613658487797\n",
      "Epoch 439, Loss: 0.008485010825097561\n",
      "Epoch 440, Loss: 0.00845662597566843\n",
      "Epoch 441, Loss: 0.008430585265159607\n",
      "Epoch 442, Loss: 0.008450130932033062\n",
      "Epoch 443, Loss: 0.008450918830931187\n",
      "Epoch 444, Loss: 0.00842316448688507\n",
      "Epoch 445, Loss: 0.008418973535299301\n",
      "Epoch 446, Loss: 0.008432414382696152\n",
      "Epoch 447, Loss: 0.00842132605612278\n",
      "Epoch 448, Loss: 0.008403807878494263\n",
      "Epoch 449, Loss: 0.008407790213823318\n",
      "Epoch 450, Loss: 0.00841240119189024\n",
      "Epoch 451, Loss: 0.008399727754294872\n",
      "Epoch 452, Loss: 0.008390570059418678\n",
      "Epoch 453, Loss: 0.008395186625421047\n",
      "Epoch 454, Loss: 0.008394504897296429\n",
      "Epoch 455, Loss: 0.008384211920201778\n",
      "Epoch 456, Loss: 0.008379503153264523\n",
      "Epoch 457, Loss: 0.008382517844438553\n",
      "Epoch 458, Loss: 0.008379856124520302\n",
      "Epoch 459, Loss: 0.008372255600988865\n",
      "Epoch 460, Loss: 0.008369363844394684\n",
      "Epoch 461, Loss: 0.008370820432901382\n",
      "Epoch 462, Loss: 0.008367963135242462\n",
      "Epoch 463, Loss: 0.008362364955246449\n",
      "Epoch 464, Loss: 0.008360092528164387\n",
      "Epoch 465, Loss: 0.00836049485951662\n",
      "Epoch 466, Loss: 0.00835811160504818\n",
      "Epoch 467, Loss: 0.008353855460882187\n",
      "Epoch 468, Loss: 0.008351728320121765\n",
      "Epoch 469, Loss: 0.008351471275091171\n",
      "Epoch 470, Loss: 0.00834969338029623\n",
      "Epoch 471, Loss: 0.008346393704414368\n",
      "Epoch 472, Loss: 0.008344284258782864\n",
      "Epoch 473, Loss: 0.008343587629497051\n",
      "Epoch 474, Loss: 0.008342297747731209\n",
      "Epoch 475, Loss: 0.008339769206941128\n",
      "Epoch 476, Loss: 0.008337688632309437\n",
      "Epoch 477, Loss: 0.008336656726896763\n",
      "Epoch 478, Loss: 0.008335655555129051\n",
      "Epoch 479, Loss: 0.008333791047334671\n",
      "Epoch 480, Loss: 0.008331848308444023\n",
      "Epoch 481, Loss: 0.008330569602549076\n",
      "Epoch 482, Loss: 0.008329642936587334\n",
      "Epoch 483, Loss: 0.008328300900757313\n",
      "Epoch 484, Loss: 0.00832662358880043\n",
      "Epoch 485, Loss: 0.008325220085680485\n",
      "Epoch 486, Loss: 0.008324215188622475\n",
      "Epoch 487, Loss: 0.008323190733790398\n",
      "Epoch 488, Loss: 0.008321858011186123\n",
      "Epoch 489, Loss: 0.008320492692291737\n",
      "Epoch 490, Loss: 0.0083193713799119\n",
      "Epoch 491, Loss: 0.008318441919982433\n",
      "Epoch 492, Loss: 0.008317405357956886\n",
      "Epoch 493, Loss: 0.008316225372254848\n",
      "Epoch 494, Loss: 0.008315078914165497\n",
      "Epoch 495, Loss: 0.008314105682075024\n",
      "Epoch 496, Loss: 0.008313211612403393\n",
      "Epoch 497, Loss: 0.008312247693538666\n",
      "Epoch 498, Loss: 0.008311213925480843\n",
      "Epoch 499, Loss: 0.00831021647900343\n",
      "Epoch 500, Loss: 0.00830932892858982\n",
      "Epoch 501, Loss: 0.00830848515033722\n",
      "Epoch 502, Loss: 0.00830761343240738\n",
      "Epoch 503, Loss: 0.008306698873639107\n",
      "Epoch 504, Loss: 0.008305808529257774\n",
      "Epoch 505, Loss: 0.008304985240101814\n",
      "Epoch 506, Loss: 0.008304200135171413\n",
      "Epoch 507, Loss: 0.008303409442305565\n",
      "Epoch 508, Loss: 0.008302595466375351\n",
      "Epoch 509, Loss: 0.008301792666316032\n",
      "Epoch 510, Loss: 0.00830102525651455\n",
      "Epoch 511, Loss: 0.008300296030938625\n"
     ]
    }
   ],
   "source": [
    "model = PINN(layers)\n",
    "\n",
    "x, t, u_true = generate_data(x_points, t_points, xmin, xmax, tmin, tmax)\n",
    "\n",
    "train(model, x, t, u_true, lambda model, x, t: pde_res(model, x, t), epochs, learning_rate)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
